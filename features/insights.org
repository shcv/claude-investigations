#+TITLE: Claude Code /insights Command
#+DESCRIPTION: AI-powered usage analytics for Claude Code sessions

* Overview

The =/insights= command generates a comprehensive AI-powered analysis of your Claude Code usage patterns. It processes your session transcripts to identify what's working well, friction points, and personalized suggestions for improvement.

* Command Details

| Property         | Value                                        |
|------------------+----------------------------------------------|
| Name             | insights                                     |
| Type             | prompt                                       |
| Source           | builtin                                      |
| Progress Message | "analyzing your sessions"                    |
| Hidden           | No                                           |

* How It Works

** 1. Session Collection and Filtering

The command first loads all sessions from =~/.claude/projects/= and applies filters:

- Excludes agent-generated sessions (names starting with "agent-")
- Excludes facet extraction sessions (those with JSON extraction prompts)
- Requires minimum 2 user messages
- Requires minimum 1 minute duration
- Analyzes up to 50 sessions (most recent first)

** 2. Facet Extraction (Per-Session)

Each session is analyzed with an LLM to extract structured "facets" - categories of goals, satisfaction signals, and friction points.

*** Facet Extraction Prompt

#+begin_src text
Analyze this Claude Code session and extract structured facets.

CRITICAL GUIDELINES:

1. **goal_categories**: Count ONLY what the USER explicitly asked for.
   - DO NOT count Claude's autonomous codebase exploration
   - DO NOT count work Claude decided to do on its own
   - ONLY count when user says "can you...", "please...", "I need...", "let's..."

2. **user_satisfaction_counts**: Base ONLY on explicit user signals.
   - "Yay!", "great!", "perfect!" → happy
   - "thanks", "looks good", "that works" → satisfied
   - "ok, now let's..." (continuing without complaint) → likely_satisfied
   - "that's not right", "try again" → dissatisfied
   - "this is broken", "I give up" → frustrated

3. **friction_counts**: Be specific about what went wrong.
   - misunderstood_request: Claude interpreted incorrectly
   - wrong_approach: Right goal, wrong solution method
   - buggy_code: Code didn't work correctly
   - user_rejected_action: User said no/stop to a tool call
   - excessive_changes: Over-engineered or changed too much

4. If very short or just warmup, use warmup_minimal for goal_category

SESSION:
#+end_src

*** Chunk Summarization Prompt

For long sessions (>25KB), chunks are summarized first:

#+begin_src text
Summarize this portion of a Claude Code session transcript. Focus on:
1. What the user asked for
2. What Claude did (tools used, files modified)
3. Any friction or issues
4. The outcome

Keep it concise - 3-5 sentences. Preserve specific details like file names, error messages, and user feedback.

TRANSCRIPT CHUNK:
#+end_src

** 3. Multi-Facet Analysis

After extracting per-session facets, aggregated data is analyzed with 8 specialized prompts:

*** Project Areas

#+begin_src text
Analyze this Claude Code usage data and identify project areas.

RESPOND WITH ONLY A VALID JSON OBJECT:
{
  "areas": [
    {"name": "Area name", "session_count": N, "description": "2-3 sentences about what was worked on and how Claude Code was used."}
  ]
}

Include 4-5 areas. Skip internal CC operations.
#+end_src

*** Interaction Style

#+begin_src text
Analyze this Claude Code usage data and describe the user's interaction style.

RESPOND WITH ONLY A VALID JSON OBJECT:
{
  "narrative": "2-3 paragraphs analyzing HOW the user interacts with Claude Code. Use second person 'you'. Describe patterns: iterate quickly vs detailed upfront specs? Interrupt often or let Claude run? Include specific examples. Use **bold** for key insights.",
  "key_pattern": "One sentence summary of most distinctive interaction style"
}
#+end_src

*** What Works

#+begin_src text
Analyze this Claude Code usage data and identify what's working well for this user. Use second person ("you").

RESPOND WITH ONLY A VALID JSON OBJECT:
{
  "intro": "1 sentence of context",
  "impressive_workflows": [
    {"title": "Short title (3-6 words)", "description": "2-3 sentences describing the impressive workflow or approach. Use 'you' not 'the user'."}
  ]
}

Include 3 impressive workflows.
#+end_src

*** Friction Analysis

#+begin_src text
Analyze this Claude Code usage data and identify friction points for this user. Use second person ("you").

RESPOND WITH ONLY A VALID JSON OBJECT:
{
  "intro": "1 sentence summarizing friction patterns",
  "categories": [
    {"category": "Concrete category name", "description": "1-2 sentences explaining this category and what could be done differently. Use 'you' not 'the user'.", "examples": ["Specific example with consequence", "Another example"]}
  ]
}

Include 3 friction categories with 2 examples each.
#+end_src

*** Suggestions

#+begin_src text
Analyze this Claude Code usage data and suggest improvements.

## CC FEATURES REFERENCE (pick from these for features_to_try):
1. **MCP Servers**: Connect Claude to external tools, databases, and APIs via Model Context Protocol.
   - How to use: Run `claude mcp add <server-name> -- <command>`
   - Good for: database queries, Slack integration, GitHub issue lookup, connecting to internal APIs

2. **Custom Skills**: Reusable prompts you define as markdown files that run with a single /command.
   - How to use: Create `.claude/skills/commit/SKILL.md` with instructions. Then type `/commit` to run it.
   - Good for: repetitive workflows - /commit, /review, /test, /deploy, /pr, or complex multi-step workflows

3. **Hooks**: Shell commands that auto-run at specific lifecycle events.
   - How to use: Add to `.claude/settings.json` under "hooks" key.
   - Good for: auto-formatting code, running type checks, enforcing conventions

4. **Headless Mode**: Run Claude non-interactively from scripts and CI/CD.
   - How to use: `claude -p "fix lint errors" --allowedTools "Edit,Read,Bash"`
   - Good for: CI/CD integration, batch code fixes, automated reviews

5. **Task Agents**: Claude spawns focused sub-agents for complex exploration or parallel work.
   - How to use: Claude auto-invokes when helpful, or ask "use an agent to explore X"
   - Good for: codebase exploration, understanding complex systems

RESPOND WITH ONLY A VALID JSON OBJECT:
{
  "claude_md_additions": [
    {"addition": "A specific line or block to add to CLAUDE.md based on workflow patterns. E.g., 'Always run tests after modifying auth-related files'", "why": "1 sentence explaining why this would help based on actual sessions", "prompt_scaffold": "Instructions for where to add this in CLAUDE.md. E.g., 'Add under ## Testing section'"}
  ],
  "features_to_try": [
    {"feature": "Feature name from CC FEATURES REFERENCE above", "one_liner": "What it does", "why_for_you": "Why this would help YOU based on your sessions", "example_code": "Actual command or config to copy"}
  ],
  "usage_patterns": [
    {"title": "Short title", "suggestion": "1-2 sentence summary", "detail": "3-4 sentences explaining how this applies to YOUR work", "copyable_prompt": "A specific prompt to copy and try"}
  ]
}

IMPORTANT for claude_md_additions: PRIORITIZE instructions that appear MULTIPLE TIMES in the user data. If user told Claude the same thing in 2+ sessions (e.g., 'always run tests', 'use TypeScript'), that's a PRIME candidate - they shouldn't have to repeat themselves.

IMPORTANT for features_to_try: Pick 2-3 from the CC FEATURES REFERENCE above. Include 2-3 items for each category.
#+end_src

*** On the Horizon

#+begin_src text
Analyze this Claude Code usage data and identify future opportunities.

RESPOND WITH ONLY A VALID JSON OBJECT:
{
  "intro": "1 sentence about evolving AI-assisted development",
  "opportunities": [
    {"title": "Short title (4-8 words)", "whats_possible": "2-3 ambitious sentences about autonomous workflows", "how_to_try": "1-2 sentences mentioning relevant tooling", "copyable_prompt": "Detailed prompt to try"}
  ]
}

Include 3 opportunities. Think BIG - autonomous workflows, parallel agents, iterating against tests.
#+end_src

*** Fun Ending

#+begin_src text
Analyze this Claude Code usage data and find a memorable moment.

RESPOND WITH ONLY A VALID JSON OBJECT:
{
  "headline": "A memorable QUALITATIVE moment from the transcripts - not a statistic. Something human, funny, or surprising.",
  "detail": "Brief context about when/where this happened"
}

Find something genuinely interesting or amusing from the session summaries.
#+end_src

*** At a Glance (Executive Summary)

This prompt receives the outputs of all previous analyses to create a cohesive summary:

#+begin_src text
You're writing an "At a Glance" summary for a Claude Code usage insights report for Claude Code users. The goal is to help them understand their usage and improve how they can use Claude better, especially as models improve.

Use this 4-part structure:

1. **What's working** - What is the user's unique style of interacting with Claude and what are some impactful things they've done? You can include one or two details, but keep it high level since things might not be fresh in the user's memory. Don't be fluffy or overly complimentary. Also, don't focus on the tool calls they use.

2. **What's hindering you** - Split into (a) Claude's fault (misunderstandings, wrong approaches, bugs) and (b) user-side friction (not providing enough context, environment issues -- ideally more general than just one project). Be honest but constructive.

3. **Quick wins to try** - Specific Claude Code features they could try from the examples below, or a workflow technique if you think it's really compelling. (Avoid stuff like "Ask Claude to confirm before taking actions" or "Type out more context up front" which are less compelling.)

4. **Ambitious workflows for better models** - As we move to much more capable models over the next 3-6 months, what should they prepare for? What workflows that seem impossible now will become possible? Draw from the appropriate section below.
#+end_src

** 4. Data Aggregation

The =o6z= function aggregates statistics across all sessions:

- Tool usage counts (Edit, Read, Bash, Grep, Glob, etc.)
- Programming languages worked with
- Git commits and pushes
- Session types and outcomes
- Satisfaction metrics
- Friction categories
- Multi-clauding analysis (parallel sessions)
- Time-of-day usage patterns

** 5. HTML Report Generation

The =YAz= function generates an interactive HTML report with:

- At a Glance summary with four key insights
- Project areas worked on
- Impressive accomplishments ("What's Working")
- Friction points analysis ("Where Things Go Wrong")
- Feature recommendations
- Future opportunities ("On the Horizon")
- Statistical visualizations (bar charts, histograms)
- Interactive checkboxes for copying CLAUDE.md suggestions

* Storage Locations

| Data Type          | Location                                         |
|--------------------+--------------------------------------------------|
| Session transcripts | =~/.claude/projects/{project}/{session}/=       |
| Session summaries  | =~/.claude/projects/.../session-memory/summary.md= |
| Facets cache       | =~/.claude/usage-data/facets/{session-id}.json= |
| HTML report        | =~/.claude/usage-data/report.html=              |

* Output

When you run =/insights=, you receive:

1. A markdown summary in the terminal with the "At a Glance" section
2. A link to the full HTML report: =file:///path/to/report.html=
3. A prompt asking if you want to dig into any section

* Technical Details

- Uses =querySource: "insights"= for API tracking
- Processes sessions in chunks (25KB max per chunk) to avoid token limits
- All analysis prompts use max 8192 output tokens
- Caches facets to avoid re-analyzing unchanged sessions
- Filters out warmup-only sessions from final analysis

* Goal Categories Tracked

| Category           | Description                    |
|--------------------+--------------------------------|
| debug_investigate  | Debug/Investigate              |
| implement_feature  | Implement Feature              |
| fix_bug            | Fix Bug                        |
| write_script_tool  | Write Script/Tool              |
| refactor_code      | Refactor Code                  |
| configure_system   | Configure System               |
| create_pr_commit   | Create PR/Commit               |
| analyze_data       | Analyze Data                   |
| understand_codebase | Understand Codebase           |
| write_tests        | Write Tests                    |
| write_docs         | Write Docs                     |
| deploy_infra       | Deploy/Infra                   |
| warmup_minimal     | Cache Warmup                   |

* Satisfaction Levels

| Level           | User Signals                        |
|-----------------+-------------------------------------|
| happy           | "Yay!", "great!", "perfect!"        |
| satisfied       | "thanks", "looks good", "that works"|
| likely_satisfied | "ok, now let's..." (continuing)    |
| dissatisfied    | "that's not right", "try again"     |
| frustrated      | "this is broken", "I give up"       |

* Friction Categories

| Category             | Description                         |
|----------------------+-------------------------------------|
| misunderstood_request | Claude interpreted incorrectly     |
| wrong_approach       | Right goal, wrong solution method   |
| buggy_code           | Code didn't work correctly          |
| user_rejected_action | User said no/stop to a tool call    |
| excessive_changes    | Over-engineered or changed too much |
| wrong_file_or_location | Edited wrong file or location      |
| slow_or_verbose      | Slow or overly verbose              |
| tool_failed          | Tool failed                         |
| user_unclear         | User was unclear                    |
| external_issue       | External issue                      |

* Version History

- Introduced in Claude Code v2.1.30 (based on source analysis)
